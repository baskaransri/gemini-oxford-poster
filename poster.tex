% Gemini theme
% https://github.com/anishathalye/gemini

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=custom,width=120,height=72,scale=1.0]{beamerposter}
\usetheme{gemini}
\usecolortheme{gemini}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{anyfontsize}

% Baskaran Customization
\usepackage{xcolor}
\usepackage{calc}
\usepackage{outlines}
\newcommand{\matr}[1]{\bm{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\usepackage{bm}
%\usepackage{cite}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\MSE}{$\mathbb{E}[\textrm{MSE}]$}

\newcommand{\errA}{$\color{Oxford_burnt_orange} \xi_{1}(\set{S}) \:$}
\newcommand{\errB}{$\color{Oxford_rich_burgundy} \xi_{2}(\set{S}) \:$}

\definecolor{Oxford_burnt_orange}{RGB}{207,122,48}
\definecolor{myorange}{RGB}{207,122,48}
\definecolor{Oxford_light_gold}{RGB}{243,222,116}
\definecolor{Oxford_rich_burgundy}{RGB}{135,36,52}


\definecolor{Oxford_dark_olive_green}{RGB}{105,145,59}
\definecolor{mygreen}{RGB}{105,145,59}
\definecolor{Oxford_bright_red}{RGB}{190,15,52}

\usepackage{geometry}
\geometry{a0paper, landscape, margin=0in}
% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
% For sepwidth = s*\paperwidth, colwidth = c*\paperwidth, N=3
% c = (1 - 4s)/3
\newlength{\sepwidth}
\newlength{\colwidth}

%\setlength{\sepwidth}{0.025\paperwidth}
%\setlength{\colwidth}{0.3\paperwidth}

\setlength{\sepwidth}{0.043\paperwidth}
\setlength{\colwidth}{0.276\paperwidth}
\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}


% ====================
% Title
% ====================

\title{On the Impact of Sample Size in Graph Signal Reconstruction}

%\author{Baskaran Sripathmanathan \inst{1} \and Xiaowen Dong \inst{1} \and Michael Bronstein \inst{1}}
\author{Baskaran Sripathmanathan  \and Xiaowen Dong  \and Michael Bronstein }

\institute[shortinst]{ University of Oxford } %\samelineand \inst{2} Another Institute}

% ====================
% Footer (optional)
% ====================

\footercontent{
  \href{baskaransri.github.io}{baskaransri.github.io} \hfill
  SampTA Conference 2023 \hfill
  \href{baskaran@robots.ox.ac.uk}{baskaran@robots.ox.ac.uk}}
% (can be left out to remove footer)

% ====================
% Logo (optional)
% ====================

% use this to include logos on the left and/or right side of the header:
 \logoright{\includegraphics[height=6cm]{impct_of_sample_size_qr.png}}
 \logoleft{\includegraphics[height=6cm]{ox_brand_cmyk_rev.pdf}}

% ====================
% Body
% ====================

\begin{document}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Introduction}
  Graph sampling \& reconstruction involves observing values on some nodes on a known graph and inferring the values on the other nodes under smoothness assumptions. We ask:
  \begin{enumerate}
      \item Do more observations always reduce average mean squared error of reconstruction  \MSE?
      \item Does noise affect this?
      \item Can we cleverly choose samples to always reduce reconstruction error?
  \end{enumerate}

  \end{block}
\vskip-0.5ex
  \begin{alertblock}{What is Graph Sampling \& Reconstruction?}
  On a social network, if you knew someone's friends were all 19, they are likely 19 years old. We can generalise this to reconstruct a signal via least squares (LS) reconstruction.
  \begin{outline}[enumerate]
      \1 Assume we know all friendships amongst the $N$ friends.
      \1 Assume friendships are symmetric.
      \1 Write the sum of squared difference between friends' ages $\vect{a}$ as $\vect{a}^{T}\matr{L}\vect{a}$.
      \1 Construct $\matr{U}_{k} \in \mathbb{R}^{N \times k}$ with columns the $k$ eigenvectors of $\matr{L} \in \mathbb{R}^{N \times N}$ corresponding to $\matr{L}$'s smallest eigenvalues. 
  \end{outline}
Observing a set $\set{S}$ of people, we can infer all ages $\hat{\vect{a}}$ by LS:
  \[
  \hat{\vect{a}} = \matr{U}_{k}\left(\left( \matr{U}_{k} \right)_{\set{S}}\right)^{\dagger} \vect{a}_{\set{S}}
  \]
  where $()_{\set{S}}$ restricts to rows corresponding to $\set{S}$, and $()^{\dagger}$ is the least-squares pseudoinverse. Under such a model, we only need $k$ observations! \cite{pesenson2008sampling}

  Different choices of \set{S} result in different condition numbers (and sensitivity to noise) for $\matr{U}_{k}\left(\left( \matr{U}_{k} \right)_{\set{S}}\right)^{\dagger}$ . We show experiments using four different methods to the right.
  \end{alertblock}

\vskip-0.5ex

  \begin{block}{Technical details about the Setting}

\begin{itemize}
    \item Underlying, clean $k$-bandlimited signal $\vect{x}$ with $k$ known
    \item Noise $\vect{\epsilon} \sim \mathcal{N}(0,\matr{I}_{N})$ (ask me about this!)
    \item Use an `optimal' scheme  to choose a vertex sample set $\set{S}$. \cite{wang2019low,jayawant2021doptimal,bai2020fast,puy2018random}
    \item Observe $\vect{y} = \vect{x} + \sigma\cdot \vect{\epsilon}$ at  $\set{S}$
    \item Unregularised least squares (LS) reconstruction
\end{itemize}
  \end{block}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Theoretical Results}
  Under unregularised LS reconstruction:
  \begin{enumerate}
      \item Without noise, extra samples never increase \MSE.
      \item With enough noise, extra samples \emph{can} increase \MSE.
      \item Optimal sampling cannot mitigate this.
  \end{enumerate}

  \end{block}

\vskip2ex

  \begin{block}{Experimental Results}
With high noise, increasing sample size can \emph{increase} error:

\begin{figure}
    \centering
    \includegraphics[scale=2.5]{BA_3_1000_bandwidth_100_SNRdbs_-10.0_samps_200_MSE_LS.png}
    \caption{Signal-to-Noise ratio of $10^{-1}$}
    \label{fig:High Noise}
\end{figure}

The peak is at the bandwidth of the signal. We can compute the threshold $\tau$, where if the signal-to-noise ratio is below $\tau$, observing another vertex will increase \MSE:

\begin{figure}
    \centering
    \includegraphics[scale=2.5]{BA_3_1000_bandwidth_100_thresholds_LS.png}
    \caption{SNR Thresholds (not in decibels)}
    \label{fig:Thresholds}
\end{figure}

The experiments presented here are on Barabasi-Albert graphs of 1000 vertices, with $k=100$.
  \end{block}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{exampleblock}{Details about the Theoretical Results}
    LS reconstruction is \emph{linear} and noise is \emph{independent} to the signal, so if
    \[
    \vect{y} = \vect{x} + \sigma \cdot \vect{\epsilon}
    \]
    and we observe $\vect{y}$ at $\set{S}$, then
    \[
    \mathbb{E}[\textrm{MSE}_{\set{S}}] = {\color{Oxford_burnt_orange}\xi_{1}(\set{S})} + \sigma {\color{Oxford_rich_burgundy}\xi_{2}(\set{S})}. \]
    \begin{itemize}
    \setlength{\itemindent}{1em}
            \item \errA is noiseless error.
            \item If \errB increases and $\sigma$ is big, then $\mathbb{E}[\textrm{MSE}_{\set{S}}]$ increases!

    \end{itemize}
            Under LS, as you add another sample, either:
            \begin{align*}
                &{\color{myorange} \xi_{1}(\set{S})} \textrm{ decreases } &&({\color{mygreen} \downarrow}) &\textrm{ and }  &&{\color{Oxford_rich_burgundy} \xi_{2}(\set{S})} \textrm{ increases } &({\color{Oxford_bright_red}\uparrow}) \textrm{ or} \\
                &{\color{myorange} \xi_{1}(\set{S})} \textrm{ stays the same } &&(=) &\textrm{ and } &&{\color{Oxford_rich_burgundy} \xi_{2}(\set{S})} \textrm{ decreases } &({\color{mygreen} \downarrow})
            \end{align*}
    \errA  can decrease up to $k$ times, for $k$ = bandwidth. 
  \end{exampleblock}

  \vskip1ex
  \begin{block}{Discussion}
  \begin{itemize}
      \item Reconstruction error can sometimes be decreased by removing a vertex from the observation set.
      \item Even if picked by an `optimal' sampling scheme!
      \end{itemize}
We conclude that:
      \begin{itemize}
      \item Certain existing results in the literature for noiseless settings may not necessarily generalise to the noisy case.
      \item This demonstrates the need to consider both sample selection and reconstruction methods at the same time -- for example, one can use a noise-aware regularisation scheme to mitigate this issue. \cite{chamon2017greedy}
  \end{itemize}
  \vskip0ex
  \end{block}

  \begin{block}{References}

    \nocite{*}
    \footnotesize{\bibliographystyle{plain}\bibliography{poster}}

  \end{block}

\end{column}

\separatorcolumn
\end{columns}
\end{frame}

\end{document}
